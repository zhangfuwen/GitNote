# 15 è¡Œä»£ç æ•™ AI å¼€ç¯å…³ç¯ï¼šç”¨ç¥ç»ç½‘ç»œåšâ€œèªæ˜çš„èŠ‚èƒ½ç®¡å®¶â€

> ä¸é è§„åˆ™ï¼Œä¸å†™ if-elseï¼Œåªé â€œè¯•é”™ + å¥–åŠ±â€ï¼Œè®© AI å­¦ä¼šæ™ºèƒ½æ§åˆ¶ç¯å…‰ã€‚  
> è¿™å°±æ˜¯**å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰** çš„é­…åŠ›ï¼

---

## ğŸ’¡ é—®é¢˜ï¼šç¯ï¼Œåˆ°åº•è¯¥ä»€ä¹ˆæ—¶å€™å¼€ï¼Ÿ

ä½ å®¶çš„ç¯è¿˜åœ¨æ‰‹åŠ¨å¼€å…³ï¼Ÿ  
æˆ–è€…ç”¨â€œæœ‰äººå°±å¼€ï¼Œæ²¡äººå°±å…³â€çš„ç®€å•é€»è¾‘ï¼Ÿ

ä½†ç°å®æ›´å¤æ‚ï¼š
- **ç™½å¤©æœ‰äººï¼Œä½†é˜³å…‰å……è¶³** â†’ ä¸ç”¨å¼€ç¯ï¼Œçœç”µï¼
- **æ·±å¤œæœ‰äººï¼Œæˆ¿é—´å¾ˆæš—** â†’ å¿…é¡»å¼€ç¯ï¼Œä½“éªŒä¼˜å…ˆï¼
- **æ²¡äººçš„æ—¶å€™** â†’ ç¯å¼€ç€å°±æ˜¯æµªè´¹ï¼

èƒ½ä¸èƒ½è®©ç³»ç»Ÿ**è‡ªå·±å­¦ä¼š**åœ¨å„ç§æƒ…å†µä¸‹åšå‡ºæœ€ä¼˜å†³ç­–ï¼Ÿ

**ç­”æ¡ˆæ˜¯ï¼šèƒ½ï¼ç”¨å¼ºåŒ–å­¦ä¹  + ç¥ç»ç½‘ç»œã€‚**

---

## ğŸ¤– æˆ‘ä»¬çš„ç›®æ ‡ï¼šè®­ç»ƒä¸€ä¸ªâ€œæ™ºèƒ½ç¯æ§ AIâ€

å®ƒè¦èƒ½çœ‹æ‡‚ä¸‰ä¸ªä¿¡æ¯ï¼ˆ**çŠ¶æ€ï¼ŒState**ï¼‰ï¼š
1. **æ˜¯å¦æœ‰äºº**ï¼ˆçº¢å¤–ä¼ æ„Ÿå™¨ï¼‰
2. **å½“å‰å…‰ç…§**ï¼ˆå…‰æ•ç”µé˜»ï¼‰
3. **ç°åœ¨å‡ ç‚¹**ï¼ˆç³»ç»Ÿæ—¶é’Ÿï¼‰

ç„¶åå†³å®šä¸€ä¸ªåŠ¨ä½œï¼ˆ**Action**ï¼‰ï¼š
- **å¼€ç¯ï¼ˆ1ï¼‰** æˆ– **å…³ç¯ï¼ˆ0ï¼‰**

è€Œå®ƒçš„â€œè€å¸ˆâ€ï¼Œåªé€šè¿‡ä¸€ä¸ªæ•°å­—â€”â€”**å¥–åŠ±ï¼ˆRewardï¼‰** æ¥å‘Šè¯‰å®ƒï¼š
> â€œä½ åˆšæ‰åšå¾—å¥½ï¼Œ+1 åˆ†ï¼â€ æˆ– â€œæµªè´¹ç”µäº†ï¼Œ-0.3 åˆ†ï¼â€

ä¹…è€Œä¹…ä¹‹ï¼ŒAI å°±å­¦ä¼šäº†**çœç”µåˆè´´å¿ƒ**çš„ç­–ç•¥ã€‚

---

## ğŸ§  æ ¸å¿ƒæ€æƒ³ï¼šç”¨ç¥ç»ç½‘ç»œä»£æ›¿äººå·¥è§„åˆ™

ä¼ ç»Ÿåšæ³•ï¼šå†™ä¸€å † if-else  
```python
if æœ‰äºº and å…‰ç…§ < é˜ˆå€¼:
    å¼€ç¯
else:
    å…³ç¯
```

å¼ºåŒ–å­¦ä¹ åšæ³•ï¼š**è®©ç¥ç»ç½‘ç»œè‡ªå·±å­¦è¿™ä¸ªâ€œé˜ˆå€¼â€å’Œâ€œé€»è¾‘â€ï¼**

> è¾“å…¥çŠ¶æ€ â†’ ç¥ç»ç½‘ç»œ â†’ è¾“å‡ºæ¯ä¸ªåŠ¨ä½œçš„â€œé¢„æœŸæ”¶ç›Šâ€ï¼ˆQå€¼ï¼‰ â†’ é€‰æ”¶ç›Šæœ€é«˜çš„åŠ¨ä½œ

è¿™å°±æ˜¯ **Deep Q-Networkï¼ˆDQNï¼‰** çš„ç²¾é«“ã€‚

---

## ğŸ’» æç®€ä»£ç æ¼”ç¤ºï¼ˆä»… 20 è¡Œæ ¸å¿ƒï¼‰

```python
import torch, torch.nn as nn, random, math

# ã€ç¥ç»ç½‘ç»œï¼šQå‡½æ•°é€¼è¿‘å™¨ã€‘
model = nn.Sequential(nn.Linear(3, 16), nn.ReLU(), nn.Linear(16, 2))
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

hour = random.randint(0, 23)

for step in range(2000):
    # ã€çŠ¶æ€ sã€‘= [æ˜¯å¦æœ‰äºº, å…‰ç…§, æ—¶é—´]
    time_norm = hour / 24.0
    daylight = max(0, math.cos(2 * math.pi * (hour - 12) / 24))
    light = min(1.0, max(0.0, daylight + random.gauss(0, 0.1)))
    occupied = 1 if random.random() < (0.3 if 22<=hour or hour<6 else 0.7) else 0
    s = torch.tensor([[float(occupied), light, time_norm]])

    # ã€åŠ¨ä½œ aã€‘Îµ-è´ªå©ªç­–ç•¥
    action = model(s).argmax().item() if random.random() > 0.1 else random.randint(0, 1)

    # ã€å¥–åŠ± rã€‘æ ¹æ®çŠ¶æ€+åŠ¨ä½œè®¡ç®—
    dark = light < 0.2
    if occupied:
        reward = 0.8 if (dark and action==1) else (-1.0 if (dark and action==0) else (-0.3 if (not dark and action==1) else 0.5))
    else:
        reward = -0.2 if action==1 else 1.0

    # ã€å­¦ä¹ ã€‘Q-learning æ›´æ–°
    target_q = model(s).clone().detach()
    target_q[0, action] = reward
    loss = nn.MSELoss()(model(s), target_q)
    optimizer.zero_grad(); loss.backward(); optimizer.step()

    hour = (hour + 1) % 24
```

> âœ… å…¨ç¨‹**æ— æ ‡ç­¾ã€æ— ç›‘ç£**ï¼Œåªé å¥–åŠ±ä¿¡å·è‡ªæˆ‘è¿›åŒ–ï¼

---

## ğŸ” æ¦‚å¿µä¸ä»£ç ä¸€ä¸€å¯¹åº”

| RL æ¦‚å¿µ | ä»£ç ä½“ç° | ä½œç”¨ |
|--------|--------|------|
| **Stateï¼ˆçŠ¶æ€ï¼‰** | `[occupied, light, time_norm]` | æè¿°å½“å‰ç¯å¢ƒ |
| **Actionï¼ˆåŠ¨ä½œï¼‰** | `action = 0 or 1` | æ™ºèƒ½ä½“çš„å†³ç­– |
| **Rewardï¼ˆå¥–åŠ±ï¼‰** | `reward = ...` | åé¦ˆä¿¡å·ï¼ŒæŒ‡å¯¼å­¦ä¹ æ–¹å‘ |
| **Policyï¼ˆç­–ç•¥ï¼‰** | `model(s).argmax()` + Îµ-æ¢ç´¢ | å¦‚ä½•é€‰åŠ¨ä½œ |
| **Q-Network** | `model = nn.Sequential(...)` | ç”¨ç¥ç»ç½‘ç»œä¼°è®¡ Q(s,a) |
| **Learning** | `loss.backward()` + `optimizer.step()` | å‚æ•°æ›´æ–°ï¼Œè¶Šå­¦è¶Šèªæ˜ |

---

## âœ… è®­ç»ƒåï¼ŒAI å­¦åˆ°äº†ä»€ä¹ˆï¼Ÿ

æˆ‘ä»¬æµ‹è¯•å‡ ä¸ªå…¸å‹åœºæ™¯ï¼š

| åœºæ™¯ | AI å†³ç­– |
|------|--------|
| ğŸŒ™ æ·±å¤œï¼ˆ3ç‚¹ï¼‰ï¼Œæœ‰äººï¼Œæˆ¿é—´å¾ˆæš— | âœ… **å¼€ç¯** |
| â˜€ï¸ æ­£åˆï¼ˆ12ç‚¹ï¼‰ï¼Œæœ‰äººï¼Œé˜³å…‰å……è¶³ | âœ… **å…³ç¯**ï¼ˆçœç”µï¼ï¼‰|
| ğŸŒƒ æ™šä¸Šï¼ˆ22ç‚¹ï¼‰ï¼Œæ— äºº | âœ… **å…³ç¯** |
| ğŸŒ† å‚æ™šï¼ˆ18ç‚¹ï¼‰ï¼Œæœ‰äººï¼Œå…‰çº¿æ˜æš— | âœ… **å¼€ç¯** |

> å®ƒæ²¡æœ‰è¢«å†™æ­»è§„åˆ™ï¼Œå´**è‡ªå·±æ€»ç»“å‡ºäº†â€œåˆç†å¼€ç¯â€çš„æ™ºæ…§**ï¼

---

## ğŸŒ è¿™ä¸åªæ˜¯ç©å…·ï¼šçœŸå®åº”ç”¨åœºæ™¯

- ğŸ¢ **æ™ºèƒ½åŠå…¬æ¥¼**ï¼šè‡ªåŠ¨è°ƒèŠ‚ç…§æ˜ã€ç©ºè°ƒï¼Œå¹´çœç”µè´¹ 20%+
- ğŸ  **æ™ºèƒ½å®¶å±…**ï¼šä¸ç±³å®¶ã€HomeKit é›†æˆï¼Œæ— æ„ŸèŠ‚èƒ½
- ğŸ« **æ•™å®¤/ä¼šè®®å®¤**ï¼šäººèµ°ç¯ç­ï¼Œäººæ¥ç¯äº®ï¼Œæ— éœ€æ”¹é€ ç”µè·¯

**æ ¸å¿ƒéƒ½æ˜¯åŒä¸€ä¸ª RL æ¡†æ¶**â€”â€”åªéœ€æ¢çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ï¼

---

## ğŸš€ ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦ï¼Ÿ

> å¼ºåŒ–å­¦ä¹ ï¼Œæ˜¯è®©æœºå™¨å…·å¤‡**è‡ªä¸»å†³ç­–èƒ½åŠ›**çš„å…³é”®æŠ€æœ¯ã€‚

å®ƒä¸ä¾èµ–äººç±»ç»éªŒå†™è§„åˆ™ï¼Œè€Œæ˜¯ï¼š
1. **æ„ŸçŸ¥ç¯å¢ƒ**ï¼ˆçŠ¶æ€ï¼‰
2. **å°è¯•è¡ŒåŠ¨**ï¼ˆåŠ¨ä½œï¼‰
3. **æ¥å—åé¦ˆ**ï¼ˆå¥–åŠ±ï¼‰
4. **è‡ªæˆ‘ä¼˜åŒ–**ï¼ˆå­¦ä¹ ï¼‰

**ä»ä¸€ç›ç¯ï¼Œåˆ°è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººã€é‡‘èäº¤æ˜“â€”â€”åº•å±‚é€»è¾‘ç›¸é€šã€‚**

---

## ğŸ’¬ ç»“è¯­

ä½ ä¸éœ€è¦é€ å‡º AlphaGoï¼Œæ‰èƒ½ç”¨ä¸Šå¼ºåŒ–å­¦ä¹ ã€‚  
**ä»ä¸€ä¸ªç¯å¼€å§‹ï¼Œä½ å°±èƒ½æ„å»ºçœŸæ­£â€œä¼šæ€è€ƒâ€çš„æ™ºèƒ½ç³»ç»Ÿã€‚**

> ä»£ç è™½å°ï¼Œæ€æƒ³å·¨å¤§ã€‚  
> çœŸæ­£çš„ AIï¼Œä¸æ˜¯æ›´å¤æ‚çš„æ¨¡å‹ï¼Œè€Œæ˜¯æ›´èªæ˜çš„é—®é¢˜å»ºæ¨¡ã€‚

---

âœ… **åŠ¨æ‰‹è¯•è¯•ï¼Ÿ**  
åªéœ€å®‰è£… PyTorchï¼Œå¤åˆ¶ä¸Šæ–¹ä»£ç ï¼Œå‡ åˆ†é’Ÿå°±èƒ½çœ‹åˆ° AI å­¦ä¼šâ€œèªæ˜å¼€ç¯â€ï¼

