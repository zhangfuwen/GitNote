# 1000è¡Œä»£ç ï¼Œè¯»æ‡‚å¤§æ¨¡å‹æ¨ç†ï¼šæ‰‹æŠŠæ‰‹è§£ællama.cï¼Œè¿™æ‰æ˜¯å­¦ä¹ LLMçš„ç»ˆæå…¥é—¨è¯¾


ä½ æ˜¯å¦æ›¾è¢«GPT-4ã€Llama 3ã€Qwenè¿™äº›â€œåºç„¶å¤§ç‰©â€å“é€€ï¼Ÿ  
ä½ æ˜¯å¦ç¿»éäº†Hugging Faceçš„æºç ï¼Œå´åœ¨å‡ åƒè¡Œçš„PyTorchå’ŒCUDAä¸­è¿·å¤±æ–¹å‘ï¼Ÿ  
ä½ æ˜¯å¦æ¸´æœ›çœŸæ­£â€œçœ‹æ‡‚â€ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹æ˜¯å¦‚ä½•è¿è¡Œçš„â€”â€”ä¸æ˜¯è°ƒAPIï¼Œè€Œæ˜¯ä»é›¶æ„å»ºå®ƒçš„æ¨ç†é€»è¾‘ï¼Ÿ

ä»Šå¤©ï¼Œæˆ‘è¦å‘ä½ æ¨èä¸€ä¸ªâ€œåå¸¸è¯†â€çš„å®è—é¡¹ç›®ï¼š**llama.c**ã€‚

å®ƒåªæœ‰**1000è¡ŒCä»£ç **ã€‚  
æ²¡æœ‰PyTorchï¼Œæ²¡æœ‰CUDAï¼Œæ²¡æœ‰ä¾èµ–åº“ã€‚  
æ²¡æœ‰å¤æ‚çš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œæ²¡æœ‰é‡åŒ–ä¼˜åŒ–ï¼Œæ²¡æœ‰TensorRTã€‚  
ä½†å®ƒï¼Œ**å®Œæ•´å®ç°äº†Llama 2çš„æ¨ç†æ ¸å¿ƒ**ã€‚

å®ƒä¸è¿½æ±‚æ€§èƒ½ï¼Œåªè¿½æ±‚**æ¸…æ™°**ã€‚  
å®ƒä¸ä¸ºç”Ÿäº§ï¼Œåªä¸º**ç†è§£**ã€‚

è€Œå®ƒçš„ä½œè€…ï¼Œæ˜¯ä¸€ä½åå«**Andrej Karpathy**çš„AIç ”ç©¶å‘˜â€”â€”ç‰¹æ–¯æ‹‰å‰AIæ€»ç›‘ã€æ–¯å¦ç¦åšå£«ã€ã€Šç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€‹è¯¾ç¨‹åˆ›å§‹äººï¼Œä¹Ÿæ˜¯ä½ å¯èƒ½åœ¨YouTubeä¸Šè§è¿‡çš„é‚£ä½â€œç”¨Pythonç”»ç¥ç»ç½‘ç»œâ€çš„æå®¢ã€‚

---

### ä¸ºä»€ä¹ˆè¯´llama.cæ˜¯å­¦ä¹ LLMçš„â€œé»„é‡‘å…¥é—¨â€ï¼Ÿ

åœ¨AIåœˆï¼Œæˆ‘ä»¬æ€»è¢«â€œå¤§æ¨¡å‹â€â€œå¤šå¡è®­ç»ƒâ€â€œå‚æ•°é‡ä¸‡äº¿â€è¿™äº›è¯åŒ…å›´ã€‚ä½†çœŸç›¸æ˜¯ï¼š**ä¸€ä¸ªTransformeræ¨¡å‹çš„æ¨ç†ï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªéå¸¸å¹²å‡€çš„æ•°å­¦æµç¨‹**ã€‚

llama.cçš„è¯ç”Ÿï¼Œå°±æ˜¯ä¸ºäº†æ’•å¼€è¿™å±‚â€œç¥ç§˜æ„Ÿâ€ã€‚

> â€œå¦‚æœä½ ä¸èƒ½ç”¨1000è¡Œä»£ç å†™å‡ºæ¥ï¼Œè¯´æ˜ä½ è¿˜æ²¡çœŸæ­£ç†è§£å®ƒã€‚â€  
> â€”â€” Andrej Karpathy

ä»–ç”¨Cè¯­è¨€ï¼Œä»å¤´å®ç°Llama 2çš„å‰å‘ä¼ æ’­ï¼ŒåŒ…æ‹¬ï¼š

- Token Embedding
- Layer Normalization
- Multi-Head Attentionï¼ˆå«RoPEä½ç½®ç¼–ç ï¼‰
- Feed-Forward Network
- Softmax + è¾“å‡ºæŠ•å½±

æ‰€æœ‰ä»£ç ï¼Œ**æ²¡æœ‰ä¸€è¡Œæ˜¯é»‘ç®±**ã€‚  
æ‰€æœ‰çŸ©é˜µè¿ç®—ï¼Œ**æ‰‹åŠ¨å±•å¼€**ã€‚  
æ‰€æœ‰å¼ é‡ç»´åº¦ï¼Œ**æ¸…æ™°æ ‡æ³¨**ã€‚  
æ‰€æœ‰æ³¨é‡Šï¼Œ**åƒè€å¸ˆè®²è¯¾ä¸€æ ·ç»†è‡´**ã€‚

ä½ ä¸éœ€è¦GPUï¼Œä¸éœ€è¦Pythonç¯å¢ƒï¼Œç”šè‡³ä¸éœ€è¦ç¼–è¯‘å™¨â€”â€”ç”¨gccå°±èƒ½è·‘èµ·æ¥ã€‚  
ä½ åªéœ€è¦ä¸€é¢—æ„¿æ„æ€è€ƒçš„å¿ƒã€‚

---

### ä»£ç é€»è¾‘æ€»è§ˆï¼šä»Tokenåˆ°Textï¼Œ1000è¡Œèµ°å®ŒLlama 2æ¨ç†å…¨æµç¨‹

llama.cçš„æ ¸å¿ƒæµç¨‹ï¼Œå¯ä»¥æ¦‚æ‹¬ä¸ºä»¥ä¸‹5æ­¥ï¼š

1. **åŠ è½½æ¨¡å‹**ï¼šè¯»å–ä»Hugging Faceå¯¼å‡ºçš„äºŒè¿›åˆ¶æƒé‡æ–‡ä»¶ï¼ˆ.binï¼‰  
2. **Tokenizeè¾“å…¥**ï¼šç”¨å†…ç½®çš„Byte-Pair Encodingï¼ˆBPEï¼‰è¯è¡¨ï¼ŒæŠŠæ–‡å­—è½¬æˆæ•°å­—åºåˆ—  
3. **å‰å‘ä¼ æ’­**ï¼šé€å±‚æ‰§è¡ŒAttention + FFNï¼Œä¼ é€’éšè—çŠ¶æ€  
4. **é‡‡æ ·è¾“å‡º**ï¼šç”¨Temperature + Top-pé‡‡æ ·ï¼Œä»æ¦‚ç‡åˆ†å¸ƒä¸­ç”Ÿæˆä¸‹ä¸€ä¸ªToken  
5. **å¾ªç¯ç”Ÿæˆ**ï¼šç›´åˆ°ç”Ÿæˆç»“æŸç¬¦æˆ–è¾¾åˆ°æœ€å¤§é•¿åº¦ï¼Œè¾“å‡ºå®Œæ•´æ–‡æœ¬

æ•´ä¸ªç³»ç»Ÿï¼Œ**æ²¡æœ‰æ¡†æ¶ä¾èµ–ï¼Œæ²¡æœ‰åŠ¨æ€å›¾ï¼Œæ²¡æœ‰è‡ªåŠ¨å¾®åˆ†**ã€‚  
ä½ çœ‹åˆ°çš„ï¼Œå°±æ˜¯æ¨¡å‹åœ¨â€œæ€è€ƒâ€æ—¶ï¼Œæ¯ä¸€æ­¥å‘ç”Ÿäº†ä»€ä¹ˆã€‚

---

### é€è¡Œè§£æï¼šllama.cæ ¸å¿ƒä»£ç æ‹†è§£

ä¸‹é¢ï¼Œæˆ‘ä»¬ä»ä¸»å‡½æ•°å¼€å§‹ï¼Œä¸€æ®µä¸€æ®µå¸¦ä½ è¯»æ‡‚è¿™1000è¡Œâ€œç¥ä½œâ€ã€‚

---

#### ğŸ“Œ 1. ä¸»å‡½æ•°å…¥å£ï¼š`main()` â€”â€” ä¸€åˆ‡ä»è¿™é‡Œå¼€å§‹

```c
int main(int argc, char *argv[]) {
    // 1. åŠ è½½æ¨¡å‹æƒé‡
    Transformer transformer;
    load_model("weights/llama-2-7b.bin", &transformer);

    // 2. åˆå§‹åŒ–è¯è¡¨
    Tokenizer tokenizer;
    load_tokenizer("weights/tokenizer.bin", &tokenizer);

    // 3. è¾“å…¥æç¤ºè¯
    char *prompt = "The capital of France is ";
    
    // 4. ç¼–ç è¾“å…¥
    int *tokens = encode(tokenizer, prompt, &num_tokens);

    // 5. å¼€å§‹ç”Ÿæˆ
    generate(transformer, tokenizer, tokens, num_tokens, 100);

    return 0;
}
```

**è§£è¯»**ï¼š  
è¿™å°±æ˜¯æ•´ä¸ªç³»ç»Ÿçš„â€œæŒ‡æŒ¥ä¸­å¿ƒâ€ã€‚æ²¡æœ‰TensorFlowï¼Œæ²¡æœ‰Jupyterï¼Œåªæœ‰æ¸…æ™°çš„å‡½æ•°è°ƒç”¨é“¾ã€‚  
ä½ ä¸€çœ¼å°±èƒ½çœ‹å‡ºï¼š**æ¨¡å‹åŠ è½½ â†’ è¾“å…¥ç¼–ç  â†’ ç”Ÿæˆè¾“å‡º**ã€‚  
æ²¡æœ‰éšè—é€»è¾‘ï¼Œæ²¡æœ‰é­”æ³•å‡½æ•°ã€‚  
**è¿™å°±æ˜¯å·¥ç¨‹çš„ç¾ã€‚**

---

#### ğŸ“Œ 2. æ¨¡å‹åŠ è½½ï¼š`load_model()` â€”â€” æƒé‡ä»æ–‡ä»¶åˆ°å†…å­˜

```c
void load_model(char *filename, Transformer *model) {
    FILE *f = fopen(filename, "rb");
    fread(&model->config, sizeof(Config), 1, f); // è¯»å–æ¨¡å‹é…ç½®
    model->wte = malloc(model->config.vocab_size * model->config.dim * sizeof(float)); // è¯åµŒå…¥
    fread(model->wte, sizeof(float), model->config.vocab_size * model->config.dim, f);
    // ... é€å±‚åŠ è½½ attentionã€ffnã€norm æƒé‡
    fclose(f);
}
```

**è§£è¯»**ï¼š  
`Config`ç»“æ„ä½“é‡Œï¼Œè®°å½•äº†æ¨¡å‹çš„å±‚æ•°ã€å¤´æ•°ã€ç»´åº¦ç­‰è¶…å‚æ•°ã€‚  
æ‰€æœ‰æƒé‡ï¼ˆè¯åµŒå…¥ã€æ³¨æ„åŠ›QKVã€å‰é¦ˆç½‘ç»œæƒé‡ï¼‰è¢«**è¿ç»­è¯»å…¥å†…å­˜**ï¼Œæ²¡æœ‰åˆ†å±‚å°è£…ã€‚  
ä½ çœ‹åˆ°çš„ä¸æ˜¯`model.transformer.h[0].attn.wq.weight`ï¼Œè€Œæ˜¯`model.wq[0]`â€”â€”**ç›´æ¥ç´¢å¼•ï¼Œæ¯«æ— æŠ½è±¡**ã€‚  
è¿™æ­£æ˜¯å­¦ä¹ çš„ç²¾é«“ï¼š**å‰¥ç¦»æ¡†æ¶ï¼Œç›´é¢æ•°æ®**ã€‚

---

#### ğŸ“Œ 3. Tokenizerï¼š`encode()` â€”â€” æ–‡å­—å¦‚ä½•å˜æˆæ•°å­—ï¼Ÿ

```c
int* encode(Tokenizer t, char *text, int *out_len) {
    int len = strlen(text);
    int *tokens = malloc(MAX_SEQ_LEN * sizeof(int));
    *out_len = 0;

    for (int i = 0; i < len; ) {
        int max_match = 0;
        int best_idx = 0;
        for (int j = 0; j < t.vocab_size; j++) {
            if (strncmp(text + i, t.vocab[j], strlen(t.vocab[j])) == 0 &&
                strlen(t.vocab[j]) > max_match) {
                max_match = strlen(t.vocab[j]);
                best_idx = j;
            }
        }
        tokens[(*out_len)++] = best_idx;
        i += max_match;
    }
    return tokens;
}
```

**è§£è¯»**ï¼š  
è¿™æ˜¯æœ€åŸå§‹çš„BPEç¼–ç å®ç°ã€‚  
å®ƒä¸ä¾èµ–Pythonçš„`transformers`åº“ï¼Œè€Œæ˜¯**æš´åŠ›éå†è¯è¡¨**ï¼Œæ‰¾æœ€é•¿åŒ¹é…ã€‚  
ä½ ç”šè‡³å¯ä»¥æ‰“å°å‡º`t.vocab[0]`ï¼Œçœ‹åˆ°`"Ä "`ï¼ˆç©ºæ ¼çš„ç‰¹æ®Šç¼–ç ï¼‰ã€`"the"`ã€`"ing"`è¿™äº›å­è¯å•å…ƒã€‚  
**ä½ ç»ˆäºæ˜ç™½ï¼šåŸæ¥â€œAIç†è§£æ–‡å­—â€ï¼Œæ˜¯ä»â€œæ‹†å­—â€å¼€å§‹çš„ã€‚**

---

#### ğŸ“Œ 4. å‰å‘ä¼ æ’­æ ¸å¿ƒï¼š`transformer_forward()` â€”â€” Attentionçš„çœŸé¢ç›®

```c
void transformer_forward(Transformer *model, int *tokens, int n_tokens, float *logits) {
    // Step 1: Embedding
    float *x = model->x; // å½“å‰éšè—çŠ¶æ€
    for (int i = 0; i < n_tokens; i++) {
        for (int j = 0; j < model->config.dim; j++) {
            x[i * model->config.dim + j] = model->wte[tokens[i] * model->config.dim + j];
        }
    }

    // Step 2: Layer-by-layer Transformer blocks
    for (int l = 0; l < model->config.n_layers; l++) {
        // RMSNorm
        rmsnorm(x, model->rms_att_weight[l], model->config.dim, n_tokens);

        // Multi-Head Attention
        attention(model, x, l, n_tokens);

        // Add residual
        for (int i = 0; i < n_tokens * model->config.dim; i++) {
            x[i] += model->residual[i];
        }

        // RMSNorm again
        rmsnorm(x, model->rms_ffn_weight[l], model->config.dim, n_tokens);

        // Feed Forward Network
        ffn(model, x, l, n_tokens);

        // Add residual again
        for (int i = 0; i < n_tokens * model->config.dim; i++) {
            x[i] += model->residual[i];
        }
    }

    // Final RMSNorm
    rmsnorm(x, model->rms_final_weight, model->config.dim, n_tokens);

    // Final projection to vocab
    matmul(x, model->wcls, logits, n_tokens, model->config.dim, model->config.vocab_size);
}
```

**è¿™æ˜¯æ•´æ®µä»£ç çš„çµé­‚ï¼**

æˆ‘ä»¬æ‹†è§£ä¸€ä¸‹ï¼š

- **Embeddingå±‚**ï¼šç”¨`tokens[i]`ä½œä¸ºç´¢å¼•ï¼ŒæŸ¥è¡¨å¾—åˆ°è¯å‘é‡ã€‚  
- **RMSNorm**ï¼šæ¯”LayerNormæ›´ç®€å•ï¼Œåªåšå‡æ–¹æ ¹å½’ä¸€åŒ–ï¼Œæ— åç½®ã€‚  
- **Attention**ï¼š  
  - è®¡ç®—Q/K/Vï¼ˆ`q = x @ wq`, `k = x @ wk`, `v = x @ wv`ï¼‰  
  - åº”ç”¨**RoPE**ï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰â€”â€”è¿™æ˜¯Llamaçš„å…³é”®åˆ›æ–°ï¼Œç”¨å¤æ•°æ—‹è½¬ä»£æ›¿ä½ç½®Embedding  
  - è®¡ç®—Attention Scoreï¼š`scores = q @ k.T / sqrt(d)`  
  - Softmax + dropoutï¼ˆè¿™é‡Œçœç•¥ï¼‰  
  - åŠ æƒæ±‚å’Œï¼š`output = scores @ v`  
  - æ‹¼æ¥å¤šå¤´ï¼ŒæŠ•å½±å›åŸç»´åº¦  
- **FFN**ï¼š`x â†’ W1 â†’ GELU â†’ W2 â†’ output`ï¼Œæ ‡å‡†ä¸¤å±‚MLP  
- **æ®‹å·®è¿æ¥**ï¼š`x = x + attention_output`ï¼Œæ¯ä¸€å±‚éƒ½åŠ å›æ¥

**ä½ çœ‹åˆ°çš„ä¸æ˜¯â€œTransformeræ¨¡å—â€ï¼Œè€Œæ˜¯æ¯ä¸€è¡ŒçŸ©é˜µä¹˜æ³•ã€æ¯ä¸€ä¸ªå¾ªç¯ã€æ¯ä¸€ä¸ªç»´åº¦å¯¹é½ã€‚**

---

#### ğŸ“Œ 5. RoPEä½ç½®ç¼–ç ï¼šLlamaçš„ä¼˜é›…è®¾è®¡

```c
void apply_rope(float *x, int head_dim, int pos, int n_heads, int seq_len) {
    for (int h = 0; h < n_heads; h++) {
        for (int i = 0; i < head_dim / 2; i++) {
            float freq = 1.0f / powf(10000.0f, 2.0f * i / head_dim);
            float theta = pos * freq;
            float cos_val = cosf(theta);
            float sin_val = sinf(theta);

            int idx1 = h * head_dim + i;
            int idx2 = h * head_dim + i + head_dim / 2;

            float x1 = x[idx1];
            float x2 = x[idx2];
            x[idx1] = x1 * cos_val - x2 * sin_val;
            x[idx2] = x1 * sin_val + x2 * cos_val;
        }
    }
}
```

**è§£è¯»**ï¼š  
è¿™æ˜¯Llama 2æœ€æƒŠè‰³çš„è®¾è®¡ä¹‹ä¸€ã€‚  
ä¼ ç»ŸTransformerç”¨å›ºå®šçš„Positional Embeddingï¼Œè€ŒRoPE**æŠŠä½ç½®ä¿¡æ¯ç¼–ç è¿›å‘é‡çš„æ—‹è½¬è§’åº¦**ã€‚  
è¿™æ®µä»£ç ï¼Œç”¨ä¸‰è§’å‡½æ•°ï¼ŒæŠŠæ¯ä¸ªè¯å‘é‡çš„å‰åŠéƒ¨åˆ†å’ŒååŠéƒ¨åˆ†è¿›è¡Œ**æ—‹è½¬**ï¼Œä»è€Œâ€œè®°ä½â€å®ƒåœ¨åºåˆ—ä¸­çš„ä½ç½®ã€‚  
**æ•°å­¦ä¹‹ç¾ï¼Œå°½åœ¨å…¶ä¸­ã€‚**

---

#### ğŸ“Œ 6. é‡‡æ ·ç”Ÿæˆï¼š`generate()` â€”â€” AIå¦‚ä½•â€œå†³å®šâ€ä¸‹ä¸€ä¸ªè¯ï¼Ÿ

```c
void generate(Transformer *model, Tokenizer tokenizer, int *tokens, int n_tokens, int max_new_tokens) {
    for (int t = 0; t < max_new_tokens; t++) {
        float *logits = malloc(model->config.vocab_size * sizeof(float));
        transformer_forward(model, tokens, n_tokens, logits);

        // Temperature sampling
        float temperature = 0.8f;
        float probs[model->config.vocab_size];
        softmax(logits, probs, model->config.vocab_size, temperature);

        // Top-p sampling
        int next_token = sample_top_p(probs, model->config.vocab_size, 0.9f);

        // Add to sequence
        tokens[n_tokens++] = next_token;

        // Print token
        char *word = decode(tokenizer, next_token);
        printf("%s", word);

        // Stop if end-of-sequence
        if (next_token == 2) break; // EOS token
        free(logits);
    }
}
```

**è§£è¯»**ï¼š  
AIä¸æ˜¯â€œçŒœâ€è¯ï¼Œæ˜¯**æŒ‰æ¦‚ç‡æŠ½æ ·**ã€‚  
- `softmax` æŠŠlogitsè½¬æˆæ¦‚ç‡åˆ†å¸ƒ  
- `temperature=0.8`ï¼šè®©åˆ†å¸ƒæ›´â€œå¹³æ»‘â€ï¼Œé¿å…è¿‡äºä¿å®ˆ  
- `top_p=0.9`ï¼šåªä»ç´¯ç§¯æ¦‚ç‡90%çš„è¯ä¸­é€‰ï¼Œé¿å…ä½æ¦‚ç‡å™ªå£°  
- `sample_top_p` å‡½æ•°ç”¨éšæœºæ•°+ç´¯è®¡å’Œå®ç°é‡‡æ ·

ä½ ç»ˆäºæ˜ç™½ï¼š**AIçš„åˆ›é€ åŠ›ï¼Œæ¥è‡ªäºæ¦‚ç‡çš„éšæœºæ€§**ã€‚

---

### ç»“è¯­ï¼š1000è¡Œï¼Œèƒœè¿‡åƒç¯‡è®ºæ–‡

llama.cä¸æ˜¯ç”¨æ¥éƒ¨ç½²çš„ï¼Œå®ƒæ˜¯ç”¨æ¥**ç†è§£**çš„ã€‚

å½“ä½ åœ¨Colabé‡Œè·‘ä¸€ä¸ª`pipeline("text-generation")`æ—¶ï¼Œä½ åªæ˜¯åœ¨è°ƒç”¨ä¸€ä¸ªé»‘ç®±ã€‚  
è€Œå½“ä½ è¯»å®Œllama.cï¼Œä½ **äº²æ‰‹æ¨å¯¼äº†Transformerçš„æ¯ä¸€å±‚**ï¼Œä½ **çœ‹åˆ°äº†RoPEå¦‚ä½•ç¼–ç ä½ç½®**ï¼Œä½ **æ˜ç™½äº†é‡‡æ ·å¦‚ä½•äº§ç”Ÿâ€œçµæ„Ÿâ€**ã€‚

è¿™ä¸æ˜¯â€œå­¦ä¹ AIâ€ï¼Œè¿™æ˜¯**æˆä¸ºAIçš„é€ ç‰©ä¸»**ã€‚

> â€œä½ ä¸å¿…æˆä¸ºä¸“å®¶æ‰èƒ½ç†è§£å®ƒã€‚  
> ä½ åªéœ€è¦ï¼Œä»æœ€ç®€å•çš„ä»£ç å¼€å§‹ã€‚â€  
> â€”â€” Andrej Karpathy

---

ğŸ“Œ **é¡¹ç›®åœ°å€**ï¼šhttps://github.com/karpathy/llama.c  
ğŸ“Œ **å»ºè®®ä½ **ï¼š  
1. ä¸‹è½½ä»£ç   
2. `gcc -O2 llama.c -o llama -lm`  
3. ä¸‹è½½7Bæ¨¡å‹æƒé‡  
4. è¿è¡Œ `./llama "The meaning of life is "`  
5. çœ‹ç€AIä¸€ä¸ªå­—ä¸€ä¸ªå­—åœ°â€œæƒ³â€å‡ºæ¥

ä½ ä¼šå‘ç°ï¼š  
**AIï¼Œä»æ¥ä¸æ˜¯é­”æ³•ã€‚**  
**å®ƒåªæ˜¯ï¼Œæ•°å­¦çš„èˆè¹ˆã€‚**

