---
title: 001. AIåŸºç¡€æ¦‚å¿µ
---
# AIåŸºç¡€æ¦‚å¿µ

## Floating point numbers

### å¯¹æ¯”

| ç‰¹æ€§   | FP32  | FP16   | TF32         | BF16              |
| ---- | ----- | ------ | ------------ | ----------------- |
| ä½æ•°   | 32    | 16     | 19ï¼ˆæ¨¡æ‹Ÿï¼‰       | 16                |
| æŒ‡æ•°ä½  | **8** | 5      | **8**        | **8**             |
| å°¾æ•°ä½  | 23    | 10     | 10           | 7                 |
| æ•°å€¼èŒƒå›´ | å¤§     | å°ï¼ˆæ˜“æº¢å‡ºï¼‰ | ä¸ FP32 ä¸€è‡´    | ä¸ FP32 ä¸€è‡´         |
| ç²¾åº¦   | é«˜     | ä¸­ç­‰     | ä¸­ç­‰           | ä½ï¼ˆæœ€å·®ï¼‰             |
| ä¸»è¦ç”¨é€” | é€šç”¨è®¡ç®—  | æ¨ç†/AMP | è®­ç»ƒåŠ é€Ÿ         | AI è®­ç»ƒ/æ¨ç†          |
| æ”¯æŒå¹³å° | å¹¿æ³›    | ä¸€èˆ¬ GPU | NVIDIA A100+ | Google TPU, Intel |

1. fp16, å³half precision floating point number, ç®€ç§°half floatï¼Œä¸€ä¸ªç¬¦å·ä½ï¼Œ5ä¸ªæŒ‡æ•°ä½ï¼Œ10ä¸ªå°¾æ•°ä½ã€‚

    fp16çš„ä¸»è¦é—®é¢˜æ˜¯æŒ‡æ•°ä½ä¸å¤Ÿï¼Œä»8é™åˆ°5ï¼Œè¡¨è¾¾çš„æ•°å€¼èŒƒå›´ä¸å¤Ÿäº†ã€‚ä»1e-127~1e+128ï¼Œé™åˆ°äº†1e-15~1e+16.

2. tf32ï¼Œå³tensor float 32ï¼Œç”±nVidiaæå‡ºï¼Œä¸€ä¸ªç¬¦å·ä½ï¼Œ8ä¸ªæŒ‡æ•°ä½ï¼Œ10ä¸ªå°¾æ•°ä½ã€‚æŒ‡æ•°ä½ä¸ªæ•°ä¸fp32ç›¸åŒï¼Œè¡¨è¾¾çš„æ•°å€¼èŒƒå›´ç›¸åŒã€‚å°¾æ•°å‡å°‘äº†ï¼Œç²¾åº¦ä¸å¦‚fp32ï¼Œä½†ä¸fp16æ˜¯ä¸€æ ·çš„ã€‚å…±ä½¿ç”¨19ä¸ªæ¯”ç‰¹ã€‚

3. bf16ï¼Œå³brain float 16ï¼Œç”±Google Brainå›¢é˜Ÿæå‡ºï¼Œä¸€ä¸ªç¬¦å·ä½ï¼Œ8ä¸ªæŒ‡æ•°ä½ï¼Œ7ä¸ªå°¾æ•°ä½ã€‚æŒ‡æ•°ä½ä¸ªæ•°ä¸fp32ç›¸åŒï¼Œè¡¨è¾¾çš„æ•°å€¼èŒƒå›´ç›¸åŒï¼Œå°¾æ•°æ›´å°‘äº†ï¼Œç²¾åº¦æ›´å·®äº†ï¼Œä¸å¦‚fp16ï¼Œä½†AIè®¡ç®—èƒ½å®¹å¿ã€‚å…±ä½¿ç”¨16æ¯”ç‰¹ã€‚

---

#### **FP32ï¼ˆSingle Precision Floatï¼‰**

- **ä½æ•°åˆ†é…**ï¼š1ä½ç¬¦å· + 8ä½æŒ‡æ•° + 23ä½å°¾æ•° = **32ä½**
- **ç‰¹ç‚¹**ï¼š
  - æ˜¯ IEEE 754 æ ‡å‡†å®šä¹‰çš„æ ‡å‡†å•ç²¾åº¦æµ®ç‚¹æ•°ã€‚
  - èŒƒå›´ï¼šçº¦ `Â±1e-38` åˆ° `Â±1e+38`
  - ç²¾åº¦ï¼šå¤§çº¦ 6~7 ä½æœ‰æ•ˆæ•°å­—
- **ç”¨é€”**ï¼šä¼ ç»Ÿç§‘å­¦è®¡ç®—ã€é€šç”¨è®¡ç®—ã€æ—©æœŸæ·±åº¦å­¦ä¹ è®­ç»ƒã€‚
- **ä¼˜ç‚¹**ï¼šç²¾åº¦é«˜ï¼Œæ•°å€¼èŒƒå›´å¤§ã€‚
- **ç¼ºç‚¹**ï¼šå ç”¨å†…å­˜å¤šï¼Œè®¡ç®—æ…¢ï¼Œå¯¹æ˜¾å­˜å¸¦å®½è¦æ±‚é«˜ã€‚

> âœ… **ä¸¾ä¾‹**ï¼šå¤§å¤šæ•°ç°ä»£ GPU åœ¨ CUDA ä¸­é»˜è®¤ä½¿ç”¨ FP32 è¿›è¡Œè®¡ç®—ã€‚

---

#### **FP16ï¼ˆHalf Precision Floatï¼‰**
- **ä½æ•°åˆ†é…**ï¼š1ä½ç¬¦å· + 5ä½æŒ‡æ•° + 10ä½å°¾æ•° = **16ä½**
- **ç‰¹ç‚¹**ï¼š
  - åŒæ ·æ˜¯ IEEE 754 æ ‡å‡†çš„ä¸€éƒ¨åˆ†ï¼ˆä½†éæ‰€æœ‰è®¾å¤‡éƒ½æ”¯æŒï¼‰ã€‚
  - èŒƒå›´ï¼šçº¦ `Â±1e-15` åˆ° `Â±1e+16`ï¼ˆè¿œå°äº FP32ï¼‰
  - ç²¾åº¦ï¼šçº¦ 3~4 ä½æœ‰æ•ˆæ•°å­—
- **é—®é¢˜**ï¼š
  - æŒ‡æ•°ä½ä» 8 é™åˆ° 5 â†’ æ•°å€¼åŠ¨æ€èŒƒå›´å¤§å¹…ç¼©å°
  - å®¹æ˜“å‘ç”Ÿ **æº¢å‡ºï¼ˆoverflowï¼‰æˆ–ä¸‹æº¢ï¼ˆunderflowï¼‰**
  - å°¾æ•°ä½è™½æœ‰ 10 ä½ï¼ˆæ¯” TF32 å¤šï¼‰ï¼Œä½†æ•´ä½“ç²¾åº¦ä»ä½äº FP32

> âš ï¸ ä¾‹å¦‚ï¼šåœ¨è®­ç»ƒä¸­ï¼Œæ¢¯åº¦å¯èƒ½å¤ªå°è€Œè¢«æˆªæ–­ä¸º 0ï¼ˆä¸‹æº¢ï¼‰ï¼Œå¯¼è‡´è®­ç»ƒå¤±è´¥ã€‚

- **ç”¨é€”**ï¼š
  - ç”¨äºæ¨ç†ï¼ˆinferenceï¼‰åœºæ™¯ï¼Œå°¤å…¶æ˜¯ç§»åŠ¨ç«¯æˆ–è¾¹ç¼˜è®¾å¤‡
  - éœ€è¦é…åˆâ€œæ··åˆç²¾åº¦è®­ç»ƒâ€æŠ€æœ¯ï¼ˆå¦‚ NVIDIA çš„ AMPï¼‰

> ğŸ“Œ è¡¥å……ï¼šè™½ç„¶ FP16 å†…å­˜èŠ‚çœä¸€åŠï¼Œä½†å› æ•°å€¼èŒƒå›´å°ï¼Œç¨³å®šæ€§å·®ï¼Œä¸é€‚åˆç›´æ¥ç”¨äºæ¨¡å‹è®­ç»ƒã€‚

---

#### **TF32ï¼ˆTensorFloat-32ï¼‰**
- **æå‡ºè€…**ï¼šNVIDIAï¼ˆAmpere æ¶æ„èµ·æ”¯æŒï¼‰
- **ä½æ•°åˆ†é…**ï¼š1ä½ç¬¦å· + 8ä½æŒ‡æ•° + 10ä½å°¾æ•° = **19ä½**ï¼ˆå®é™…å­˜å‚¨ä»æ˜¯ 32 ä½ï¼Œä½†åªç”¨å‰ 19 ä½åšæœ‰æ•ˆè¿ç®—ï¼‰
- **è®¾è®¡æ€æƒ³**ï¼š
  - ä¿ç•™äº† **FP32 çš„æŒ‡æ•°ä½ï¼ˆ8ä½ï¼‰** â†’ ç»´æŒç›¸åŒçš„æ•°å€¼èŒƒå›´ï¼ˆ`Â±1e-127 ~ Â±1e+128`ï¼‰
  - åªæœ‰ **10 ä½å°¾æ•°**ï¼ˆæ¯” FP32 çš„ 23 ä½å°‘ï¼‰â†’ ç²¾åº¦ä¸‹é™ï¼Œä½†æ¯” FP16 æ›´å¥½
- **ä¼˜åŠ¿**ï¼š
  - ä¿æŒ FP32 çš„æ•°å€¼ç¨³å®šæ€§ï¼ˆä¸æ˜“æº¢å‡º/ä¸‹æº¢ï¼‰
  - æ¯” FP32 å¿«ï¼ˆå› ä¸ºå‡å°‘å°¾æ•°ä½ï¼Œä¹˜æ³•ç­‰æ“ä½œæ›´å¿«ï¼‰
  - æ”¯æŒåŸç”Ÿ 32 ä½å¯„å­˜å™¨ï¼Œæ— éœ€é¢å¤–è½¬æ¢
- **åº”ç”¨åœºæ™¯**ï¼š
  - æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­åŠ é€ŸçŸ©é˜µä¹˜æ³•ï¼ˆGEMMï¼‰
  - NVIDIA A100 / H100 GPU é»˜è®¤å¯ç”¨ TF32 åŠ é€Ÿ

> âœ… **å…³é”®ç‚¹**ï¼šTF32 æ˜¯ä¸€ç§â€œç¡¬ä»¶ä¼˜åŒ–â€çš„ä¸­é—´æ ¼å¼ï¼Œä¸æ˜¯æ ‡å‡†æµ®ç‚¹æ ¼å¼ï¼Œä»…åœ¨ç‰¹å®š GPU ä¸Šå¯ç”¨ã€‚

---

#### **BF16ï¼ˆBrain Float 16ï¼‰**
- **æå‡ºè€…**ï¼šGoogle Brain å›¢é˜Ÿ
- **ä½æ•°åˆ†é…**ï¼š1ä½ç¬¦å· + 8ä½æŒ‡æ•° + 7ä½å°¾æ•° = **16ä½**
- **è®¾è®¡æ€æƒ³**ï¼š
  - ä¿ç•™ **FP32 çš„æŒ‡æ•°ä½ï¼ˆ8ä½ï¼‰** â†’ ä¿æŒç›¸åŒæ•°å€¼èŒƒå›´
  - å°¾æ•°åªæœ‰ 7 ä½ â†’ ç²¾åº¦æ›´ä½ï¼Œä¸å¦‚ FP16
- **ä¼˜åŠ¿**ï¼š
  - ä¸ FP32 ç›¸åŒçš„åŠ¨æ€èŒƒå›´ â†’ ä¸å®¹æ˜“å‡ºç°ä¸‹æº¢/æº¢å‡º
  - é€‚åˆ AI è®¡ç®—ï¼šAI å¯¹ç²¾åº¦å®¹å¿åº¦è¾ƒé«˜ï¼Œæ›´å…³å¿ƒé€Ÿåº¦å’Œå†…å­˜æ•ˆç‡
- **åŠ£åŠ¿**ï¼š
  - ç²¾åº¦æ¯” FP16 å·®ï¼ˆå°¾æ•°æ›´å°‘ï¼‰ï¼Œä¸é€‚åˆä¼ ç»Ÿæ•°å€¼è®¡ç®—
- **ç”¨é€”**ï¼š
  - Google TPUï¼ˆå¼ é‡å¤„ç†å•å…ƒï¼‰åŸç”Ÿæ”¯æŒ
  - Intel CPU å’Œéƒ¨åˆ† FPGA æ”¯æŒ
  - å¸¸ç”¨äºæ·±åº¦å­¦ä¹ è®­ç»ƒå’Œæ¨ç†

> ğŸ” **å¯¹æ¯”**ï¼š
| æ ¼å¼   | æŒ‡æ•°ä½ | å°¾æ•°ä½ | æ€»ä½æ•° | åŠ¨æ€èŒƒå›´ | ç²¾åº¦ |
|--------|--------|--------|--------|-----------|-------|
| FP32   | 8      | 23     | 32     | å¾ˆå¤§      | é«˜    |
| FP16   | 5      | 10     | 16     | å°        | ä¸­ç­‰  |
| TF32   | 8      | 10     | 19     | å¤§        | ä¸­ç­‰  |
| BF16   | 8      | 7      | 16     | å¤§        | ä½    |

> ğŸ’¡ æ‰€ä»¥è¯´ï¼š**BF16 çš„ç²¾åº¦æ¯” FP16 å·®ï¼Œä½†æ¯” FP16 æ›´ç¨³å®š**ï¼ˆå› ä¸ºæŒ‡æ•°ä½ä¸€æ ·ï¼‰ã€‚

---

### âœ… å®é™…åº”ç”¨å»ºè®®ï¼š

| åœºæ™¯             | æ¨èæ ¼å¼       | è¯´æ˜ |
|------------------|----------------|------|
| æ·±åº¦å­¦ä¹ è®­ç»ƒ     | **BF16 æˆ– TF32** | ç¨³å®š + å¿«é€Ÿï¼›NVIDIA æ¨è TF32ï¼ŒGoogle æ¨è BF16 |
| æ¨ç†ï¼ˆéƒ¨ç½²ï¼‰     | **FP16 / BF16** | å†…å­˜å°ï¼Œé€Ÿåº¦å¿«ï¼Œå¯æ¥å—è¾ƒä½ç²¾åº¦ |
| ç§‘å­¦è®¡ç®—         | **FP32**       | å¿…é¡»ä¿è¯é«˜ç²¾åº¦ |
| è¾¹ç¼˜è®¾å¤‡         | **FP16**       | å†…å­˜å—é™ï¼Œç²¾åº¦å…è®¸ç‰ºç‰² |

## Tensor

é€šå¸¸TensoræŒ‡å››ç»´çŸ©é˜µï¼Œæ¯ä¸ªç»´åº¦åˆ†åˆ«ä¸º[N,C,W,H]ï¼Œå³batch, channel, width, heightã€‚ä¾‹å¦‚ï¼Œåœ¨å¤„ç†ä¸€æ‰¹å›¾åƒæ—¶ï¼ŒNå¯èƒ½è¡¨ç¤ºåŒæ—¶å¤„ç†çš„å›¾åƒæ•°é‡ï¼Œæ¯”å¦‚32å¼ å›¾ç‰‡ï¼›Cè¡¨ç¤ºé¢œè‰²é€šé“ï¼Œå¦‚RGBå›¾åƒçš„3ä¸ªé€šé“ï¼›Wå’ŒHåˆ™åˆ†åˆ«è¡¨ç¤ºå›¾åƒçš„å®½åº¦å’Œé«˜åº¦åƒç´ å€¼ï¼Œæ¯”å¦‚224x224ã€‚åœ¨CNNä¸­ï¼ŒNè¡¨ç¤ºbatchï¼Œæ–¹ä¾¿ç†è§£æ—¶å¯ä»¥è®¤ä¸ºå®ƒæ˜¯1ï¼Œå°±æ˜¯è¯´å®ƒå®é™…æ˜¯3ç»´çŸ©é˜µï¼Œä½†è¿™ä»…é€‚ç”¨äºå•æ ·æœ¬å¤„ç†åœºæ™¯ï¼Œå®é™…è®­ç»ƒä¸­batché€šå¸¸å¤§äº1ä»¥æå‡æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

å¯ä»¥çœ‹åˆ°ï¼Œè¿™ç§è¡¨è¿°å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ä¸ºå›¾åƒè®¾è®¡çš„ï¼Œä¸é€‚ç”¨äºå£°éŸ³å’ŒLLMã€‚å¯¹äºå£°éŸ³æ•°æ®ï¼Œä¾‹å¦‚éŸ³é¢‘ä¿¡å·ï¼Œé€šå¸¸ä½¿ç”¨ä¸€ç»´æˆ–äºŒç»´å¼ é‡ï¼Œå¦‚[batch, time_steps, frequency_bins]æˆ–[batch, channels, time]ï¼Œå…¶ä¸­æ—¶é—´æ­¥å’Œé¢‘ç‡ç®±å–ä»£äº†ç©ºé—´ç»´åº¦ã€‚å¯¹äºLLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰ï¼Œè¾“å…¥å¾€å¾€æ˜¯åºåˆ—æ•°æ®ï¼Œå¼ é‡ç»“æ„å¯èƒ½ä¸º[batch, sequence_length, embedding_dim]ï¼Œå…¶ä¸­åºåˆ—é•¿åº¦è¡¨ç¤ºè¯å…ƒæ•°é‡ï¼ŒåµŒå…¥ç»´åº¦æ•è·è¯­ä¹‰ä¿¡æ¯ï¼Œå®Œå…¨æ²¡æœ‰ç©ºé—´ç»´åº¦çš„æ¦‚å¿µã€‚å› æ­¤ï¼ŒTensorçš„ç»´åº¦å®šä¹‰éœ€è¦æ ¹æ®å…·ä½“åº”ç”¨çµæ´»è°ƒæ•´ï¼Œè€Œéå±€é™äºå›¾åƒå¤„ç†çš„ä¼ ç»Ÿæ¡†æ¶ã€‚

åœ¨LLMï¼ˆLarge Language Modelï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼‰åœºæ™¯ä¸­ï¼Œè¾“å…¥å’Œè¾“å‡ºé€šå¸¸ä»¥åºåˆ—åŒ–çš„å¼ é‡å½¢å¼å¤„ç†ã€‚è¾“å…¥å½¢çŠ¶ä¸€èˆ¬ä¸º[batch_size, sequence_length, hidden_dim]ï¼Œå…¶ä¸­batch_sizeè¡¨ç¤ºæ‰¹å¤„ç†å¤§å°ï¼Œsequence_lengthæ˜¯æ–‡æœ¬åºåˆ—çš„é•¿åº¦ï¼ˆå¦‚512æˆ–1024ä¸ªtokenï¼‰ï¼Œhidden_dimä»£è¡¨æ¯ä¸ªtokençš„åµŒå…¥ç»´åº¦ï¼ˆä¾‹å¦‚768æˆ–1024ï¼‰ã€‚è¾“å‡ºå½¢çŠ¶ä¸è¾“å…¥ä¸€è‡´ï¼Œä½†å¯èƒ½é€šè¿‡çº¿æ€§å±‚è°ƒæ•´ç»´åº¦ï¼Œä¾‹å¦‚åœ¨åˆ†ç±»ä»»åŠ¡ä¸­è¾“å‡º[batch_size, sequence_length, vocab_size]çš„æ¦‚ç‡åˆ†å¸ƒã€‚

çŸ©é˜µè¿ç®—ï¼ˆå¦‚çº¿æ€§å˜æ¢ï¼‰å¯é€šè¿‡å·ç§¯å®ç°è½¬æ¢ï¼šä¾‹å¦‚ï¼Œå…¨è¿æ¥å±‚ï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰å¯è§†ä¸º1x1å·ç§¯ï¼Œå…¶ä¸­å·ç§¯æ ¸çš„è¾“å…¥å’Œè¾“å‡ºé€šé“æ•°å¯¹åº”çŸ©é˜µçš„ç»´åº¦ã€‚å…·ä½“åœ°ï¼Œè‹¥æœ‰ä¸€ä¸ªæƒé‡çŸ©é˜µW of shape [output_dim, input_dim]ï¼Œå¯é‡å¡‘ä¸ºå·ç§¯æ ¸å½¢çŠ¶[output_dim, input_dim, 1, 1]ï¼Œä»è€Œå¯¹è¾“å…¥å¼ é‡åº”ç”¨1x1å·ç§¯ï¼Œç­‰æ•ˆäºçŸ©é˜µä¹˜æ³•ã€‚è¿™åˆ©ç”¨äº†å·ç§¯çš„å±€éƒ¨æ€§å’Œå‚æ•°å…±äº«ç‰¹æ€§ï¼Œä½†ç‰ºç‰²äº†å…¨å±€è¿æ¥æ€§ï¼Œé€‚ç”¨äºæŸäº›ç¡¬ä»¶ä¼˜åŒ–ã€‚

åœ¨LLMä¸­ï¼Œä»¥ä¸‹ç®—å­é€šå¸¸å¿…é¡»åœ¨HVXï¼ˆHexagon Vector eXtensionsï¼Œé«˜é€šçš„DSPå‘é‡åŠ é€Ÿå•å…ƒï¼‰ä¸­è®¡ç®—ä»¥æå‡æ•ˆç‡ï¼š
1. **æ¿€æ´»å‡½æ•°**ï¼šå¦‚ReLUã€GELUæˆ–Sigmoidï¼ŒHVXçš„å‘é‡æŒ‡ä»¤èƒ½é«˜æ•ˆå¤„ç†é€å…ƒç´ æ“ä½œã€‚
2. **å½’ä¸€åŒ–å±‚**ï¼šå¦‚LayerNormï¼Œæ¶‰åŠå‡å€¼ã€æ–¹å·®è®¡ç®—å’Œç¼©æ”¾ï¼ŒHVXå¯ä¼˜åŒ–è¿™äº›ç»Ÿè®¡æ“ä½œã€‚
3. **å…ƒç´ çº§è¿ç®—**ï¼šå¦‚æ®‹å·®è¿æ¥ä¸­çš„åŠ æ³•æˆ–ä¹˜æ³•ï¼ŒHVXçš„å‘é‡åŒ–æ”¯æŒé«˜æ€§èƒ½å¤„ç†ã€‚

è¿™äº›ç®—å­åœ¨HVXä¸­æ‰§è¡Œå¯é™ä½CPUè´Ÿè½½ï¼Œæå‡èƒ½æ•ˆæ¯”ï¼Œå°¤å…¶é€‚åˆç§»åŠ¨ç«¯æˆ–è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ã€‚


## Yolo

1. ä¸€èˆ¬è¿™æ¨¡å‹åˆ†ä¸‰ä¸ªéƒ¨åˆ†ï¼Œbackbone, head, anchorsã€‚backboneæ˜¯æ ¸å¿ƒéƒ¨åˆ†ï¼Œç”¨äºè¯†åˆ«å›¾åƒçš„ç‰¹å¾ã€‚headæ ¹æ®è¿™äº›ç‰¹å¾å¯¹å›¾ç‰‡è¿›è¡Œåˆ†ç±»ã€‚anchorsæ²¡æ‡‚ã€‚
2. yolov5çš„back boneæ˜¯CSPDarknet-53, CSPå…ˆä¸è§£é‡Šã€‚
3. Darknet-53æŒ‡æœ‰53ä¸ªå·ç§¯å±‚çš„darknetï¼Œå³darknetçš„ä¸€ç§é…ç½®å½¢å¼ã€‚
4. Darknet æ˜¯é¡¹ç›®çš„åå­—ï¼Œå®ƒåŒ…å«ä¸€ä¸ªæ¨¡å‹å’Œä¸€ç³»åˆ—å·¥å…·ã€‚

```
 Input
   |
Conv2D
   |
Conv2D
   |
Residual Block x 1
   |
Residual Block x 2
   |
Residual Block x 8
   |
Residual Block x 8
   |
Residual Block x 4
   |
Conv2D
   |
Conv2D
   |
   FC
   |
Output
   
```

Darknet-53ç½‘ç»œï¼š(å…¶ä¸­ä¸€ä¸ªæ®‹å·®å—åŒ…å«ä¸¤ä¸ªå·ç§¯å±‚ï¼‰

```python
   import torch
   import torch.nn as nn

class Darknet53(nn.Module):
    def __init__(self):
        super(Darknet53, self).__init__()

        # å·ç§¯å±‚1
        self.conv1 = nn.Conv2d(3, 32, 3, 1, 1)
        self.bn1 = nn.BatchNorm2d(32)
        self.relu1 = nn.LeakyReLU(0.1)

        # å·ç§¯å±‚2
        self.conv2 = nn.Conv2d(32, 64, 3, 2, 1)
        self.bn2 = nn.BatchNorm2d(64)
        self.relu2 = nn.LeakyReLU(0.1)

        # æ®‹å·®å—1
        self.resblock1 = self._make_resblock(64, 32, 64)

        # å·ç§¯å±‚3
        self.conv3 = nn.Conv2d(64, 128, 3, 2, 1)
        self.bn3 = nn.BatchNorm2d(128)
        self.relu3 = nn.LeakyReLU(0.1)

        # æ®‹å·®å—2
        self.resblock2 = self._make_resblock(128, 64, 128)
        self.resblock3 = self._make_resblock(128, 64, 128)

        # å·ç§¯å±‚4
        self.conv4 = nn.Conv2d(128, 256, 3, 2, 1)
        self.bn4 = nn.BatchNorm2d(256)
        self.relu4 = nn.LeakyReLU(0.1)

        # æ®‹å·®å—3
        self.resblock4 = self._make_resblock(256, 128, 256)
        self.resblock5 = self._make_resblock(256, 128, 256)
        self.resblock6 = self._make_resblock(256, 128, 256)
        self.resblock7 = self._make_resblock(256, 128, 256)
        self.resblock8 = self._make_resblock(256, 128, 256)
        self.resblock9 = self._make_resblock(256, 128, 256)
        self.resblock10 = self._make_resblock(256, 128, 256)
        self.resblock11 = self._make_resblock(256, 128, 256)

        # å·ç§¯å±‚5
        self.conv5 = nn.Conv2d(256, 512, 3, 2, 1)
        self.bn5 = nn.BatchNorm2d(512)
        self.relu5 = nn.LeakyReLU(0.1)

        # æ®‹å·®å—4
        self.resblock12 = self._make_resblock(512, 256, 512)
        self.resblock13 = self._make_resblock(512, 256, 512)
        self.resblock14 = self._make_resblock(512, 256, 512)
        self.resblock15 = self._make_resblock(512, 256, 512)
        self.resblock16 = self._make_resblock(512, 256, 512)
        self.resblock17 = self._make_resblock(512, 256, 512)
        self.resblock18 = self._make_resblock(512, 256, 512)
        self.resblock19 = self._make_resblock(512, 256, 512)

        # å·ç§¯å±‚6
        self.conv6 = nn.Conv2d(512, 1024, 3, 2, 1)
        self.bn6 = nn.BatchNorm2d(1024)
        self.relu6 = nn.LeakyReLU(0.1)

        # æ®‹å·®å—5
        self.resblock20 = self._make_resblock(1024, 512, 1024)
        self.resblock21 = self._make_resblock(1024, 512, 1024)
        self.resblock22 = self._make_resblock(1024, 512, 1024)
        self.resblock23 = self._make_resblock(1024, 512, 1024)

        # å¹³å‡æ± åŒ–å±‚
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))

    def forward(self, x):
        x = self.relu1(self.bn1(self.conv1(x)))
        x = self.relu2(self.bn2(self.conv2(x)))
        x = self.resblock1(x)
        x = self.relu3(self.bn3(selfconv3(x)))
        x = self.resblock2(x)
        x = self.resblock3(x)
        x = self.relu4(self.bn4(self.conv4(x)))
        x = self.resblock4(x)
        x = self.resblock5(x)
        x = self.resblock6(x)
        x = self.resblock7(x)
        x = self.resblock8(x)
        x = self.resblock9(x)
        x = self.resblock10(x)
        x = self.resblock11(x)
        x = self.relu5(self.bn5(self.conv5(x)))
        x = self.resblock12(x)
        x = self.resblock13(x)
        x = self.resblock14(x)
        x = self.resblock15(x)
        x = self.resblock16(x)
        x = self.resblock17(x)
        x = self.resblock18(x)
        x = self.resblock19(x)
        x = self.relu6(self.bn6(self.conv6(x)))
        x = self.resblock20(x)
        x = self.resblock21(x)
        x = self.resblock22(x)
        x = self.resblock23(x)
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)

        return x

    def _make_resblock(self, inplanes, planes, outplanes):
        return nn.Sequential(
            nn.Conv2d(inplanes, planes, 1, 1, 0),
            nn.BatchNorm2d(planes),
            nn.LeakyReLU(0.1),
            nn.Conv2d(planes, outplanes, 3, 1, 1),
            nn.BatchNorm2d(outplanes),
            nn.LeakyReLU(0.1),
        )
```

æ®‹å·®å—é•¿è¿™æ ·ï¼š

```python
import torch
import torch.nn as nn


class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        # å¦‚æœè¾“å…¥è¾“å‡ºé€šé“æ•°ä¸åŒï¼Œéœ€è¦ä½¿ç”¨1x1å·ç§¯æ ¸è¿›è¡Œè°ƒæ•´
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
        self.shortcut = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
            nn.BatchNorm2d(out_channels)
        )

    def forward(self, x):
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out  += self.shortcut(x)
        out = self.relu(out)
        return out


class ResNet(nn.Module):
    def __init__(self, block, num_blocks, num_classes=10):
        super(ResNet, self).__init__()
        self.in_channels = 16
        self.conv = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn = nn.BatchNorm2d(16)
        self.relu = nn.ReLU(inplace=True)
        self.layer1 = self.make_layer(block, 16, num_blocks[0], stride=1)
        self.layer2 = self.make_layer(block, 32, num_blocks[1], stride=2)
        self.layer3 = self.make_layer(block, 64, num_blocks[2], stride=2)
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, num_classes)

    def make_layer(self, block, out_channels, num_blocks, stride):
        layers = []
        layers.append(block(self.in_channels, out_channels, stride))
        self.in_channels = out_channels
        for i in range(num_blocks - 1):
        layers.append(block(out_channels, out_channels, stride=1))
        return nn.Sequential(*layers)

    def forward(self, x):
        out = self.relu(self.bn(self.conv(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.avg_pool(out)
        out = out.view(out.size(0), -1)
        out = self.fc(out)
        return out

    def ResNet18():
        return ResNet(ResidualBlock, [2, 2, 2])
        torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
```

![img_1.png](assets/img_1.png)

æ¯ä¸€ä¸ªå·ç§¯å±‚æœ‰Cin * Coutä¸ªå·ç§¯æ ¸ï¼Œè¾“å‡ºfeaturemapä¸Š[c,w,h]ä¸Šçš„å€¼æ˜¯$\sum\limits_{k=0}^{C_{in}} weight(j,k)*input(1,k)$ï¼Œå¦‚æœæœ‰biasï¼Œç›´æ¥ç›¸åŠ ã€‚jæ˜¯è¾“å‡ºchannelå·ã€‚kæ˜¯è¾“å…¥channelå·ã€‚inputçš„å½¢çŠ¶[ N,C,W,H]åˆ†åˆ«ä¸º[1, Cin, Win,  Hin]ï¼Œweightçš„å½¢çŠ¶ä¸º[N,C,W,H]åˆ†åˆ«ä¸º[Cout, Cin, 3,3], 3æ˜¯kernel sizeã€‚

5. ä¸Darknetå¹¶ä¸¾çš„è¿˜æ˜¯ResNet, æ®‹å·®ç½‘ç»œï¼š
   æ®‹å·®ç½‘ç»œä¹Ÿæœ‰ä¸€äº›å¸¸ç”¨çš„é…ç½®å½¢å¼ï¼š
   é™¤äº†ResNet18ä¹‹å¤–ï¼ŒResNetè¿˜æœ‰ä¸€ç³»åˆ—æ·±åº¦ä¸åŒçš„ç½‘ç»œï¼ŒåŒ…æ‹¬ResNet34ã€ResNet50ã€ResNet101å’ŒResNet152ç­‰ã€‚è¿™äº›ç½‘ç»œçš„å±‚æ•°åˆ†åˆ«ä¸º34ã€50ã€101å’Œ152å±‚ï¼Œå…¶ä¸­ResNet50æ˜¯æœ€å¸¸ç”¨çš„ä¸€ä¸ªç‰ˆæœ¬ã€‚
6. è¿˜æœ‰ä¸€äº›å…¶ä»–çš„å·ç§¯ç¥ç»å…ƒç½‘ç»œ
   é™¤äº†ResNetå’ŒDarknetä¹‹å¤–ï¼Œè¿˜æœ‰å¾ˆå¤šå…¶ä»–çš„æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼Œä¸‹é¢åˆ—ä¸¾ä¸€äº›æ¯”è¾ƒæµè¡Œçš„æ¨¡å‹ï¼š

VGGï¼šVGGæ˜¯ç”±ç‰›æ´¥å¤§å­¦çš„ç ”ç©¶å›¢é˜Ÿæå‡ºçš„ä¸€ç§æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼Œå…¶ä¸»è¦ç‰¹ç‚¹æ˜¯é‡‡ç”¨ä¸€ç³»åˆ—å…·æœ‰ç›¸åŒçš„å·ç§¯å±‚å’Œæ± åŒ–å±‚ç»„æˆçš„å—æ¥æ„å»ºç½‘ç»œï¼Œåœ¨ImageNetç­‰æ•°æ®é›†ä¸Šå–å¾—äº†å¾ˆå¥½çš„è¡¨ç°ã€‚

Inceptionï¼šInceptionæ˜¯ç”±è°·æ­Œçš„ç ”ç©¶å›¢é˜Ÿæå‡ºçš„ä¸€ç§æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼Œå…¶ä¸»è¦ç‰¹ç‚¹æ˜¯é‡‡ç”¨å¤šä¸ªä¸åŒå¤§å°å’Œä¸åŒç»“æ„çš„å·ç§¯æ ¸æ¥æå–ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨å¹¶è¡Œçš„ç»“æ„æ¥åŠ é€Ÿè®¡ç®—ã€‚

MobileNetï¼šMobileNetæ˜¯ç”±è°·æ­Œçš„ç ”ç©¶å›¢é˜Ÿæå‡ºçš„ä¸€ç§è½»é‡çº§æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼Œå…¶ä¸»è¦ç‰¹ç‚¹æ˜¯é‡‡ç”¨æ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼ˆdepthwise separable convolutionï¼‰æ¥å‡å°‘è®¡ç®—é‡å’Œå‚æ•°æ•°é‡ï¼Œé€‚åˆåœ¨ç§»åŠ¨è®¾å¤‡ç­‰èµ„æºæœ‰é™çš„åœºæ™¯ä¸­åº”ç”¨ã€‚

EfficientNetï¼šEfficientNetæ˜¯ç”±è°·æ­Œçš„ç ”ç©¶å›¢é˜Ÿæå‡ºçš„ä¸€ç§åŸºäºè‡ªåŠ¨åŒ–ç¥ç»ç½‘ç»œç»“æ„æœç´¢çš„æ–¹æ³•æ¥æ„å»ºé«˜æ•ˆçš„æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼Œæ—¢è€ƒè™‘äº†æ¨¡å‹æ·±åº¦ã€å®½åº¦å’Œåˆ†è¾¨ç‡ç­‰å› ç´ ï¼ŒåŒæ—¶ä¹Ÿå–å¾—äº†å¾ˆå¥½çš„è¡¨ç°ã€‚

7. æ®‹å·®ç½‘ç»œåŒ…å«å¤šä¸ªæ®‹å·®å—ï¼Œæ¯ä¸ªæ®‹å·®å—æ˜¯ä¸¤ä¸ªconv2d,åªä¸è¿‡ç¬¬ä¸€ä¸ªconv2dä¸å…‰æ¥å—ç¬¬ä¸€ä¸ªconv2dçš„è¾“å‡ºï¼Œè¿˜æ¥å—å®ƒçš„è¾“å…¥ã€‚
```python
def _make_resblock(self, inplanes, planes, outplanes):
   return nn.Sequential(
   nn.Conv2d(inplanes, planes, 1, 1, 0),
   nn.BatchNorm2d(planes),
   nn.LeakyReLU(0.1),
   nn.Conv2d(planes, outplanes, 3, 1, 1),
   nn.BatchNorm2d(outplanes),
   nn.LeakyReLU(0.1),
   )
```



## CNN

![img_1.png](assets/img_1.png)

æ¯ä¸€ä¸ªå·ç§¯å±‚æœ‰Cin * Coutä¸ªå·ç§¯æ ¸ï¼Œè¾“å‡ºfeaturemapä¸Š[c,w,h]ä¸Šçš„å€¼æ˜¯$\sum\limits_{k=0}^{C_{in}} weight(j,k)*input(1,k)$ï¼Œå¦‚æœæœ‰biasï¼Œç›´æ¥ç›¸åŠ ã€‚jæ˜¯è¾“å‡ºchannelå·ã€‚kæ˜¯è¾“å…¥channelå·ã€‚inputçš„å½¢çŠ¶[ N,C,W,H]åˆ†åˆ«ä¸º[1, Cin, Win,  Hin]ï¼Œweightçš„å½¢çŠ¶ä¸º[N,C,W,H]åˆ†åˆ«ä¸º[Cout, Cin, 3,3], 3æ˜¯kernel sizeã€‚


